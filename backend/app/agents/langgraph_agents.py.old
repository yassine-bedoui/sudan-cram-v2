from datetime import datetime
import json
import os

from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

from app.agents.state import SudanCRAMState
from app.services.vector_store import VectorStore


# ---- 1. LLM + Vector Store setup ----

llm = ChatOllama(
    model=os.getenv("OLLAMA_MODEL", "llama3.2:8b"),
    base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
    temperature=0.7,
)

vector_store = VectorStore()


# ---- 2. Nodes ----

def rag_retrieval_node(state: SudanCRAMState) -> SudanCRAMState:
    """Retrieve relevant events from Qdrant for the given region."""
    print("\nðŸ” RAG Retrieval...")

    region = state["region"]

    results = vector_store.semantic_search(
        query=f"conflict events in {region}",
        filters={"region": region},
        top_k=20,
    )

    state["retrieved_events"] = results
    state["messages"].append(f"Retrieved {len(results)} events for region {region}")

    return state


def event_extractor_node(state: SudanCRAMState) -> SudanCRAMState:
    """Extract structured events from raw text, using retrieved events as context."""
    print("\nðŸ“ Event Extractor...")

    raw = state.get("raw_data")
    if not raw:
        state["extracted_events"] = None
        state["messages"].append("No raw_data provided; skipping event extraction")
        return state

    # Use a few retrieved events as context for the LLM
    context_lines = []
    for e in state.get("retrieved_events", [])[:5]:
        meta = e.get("metadata", {})
        event_type = meta.get("event_type", "unknown")
        actors = meta.get("actors", [])
        context_lines.append(f"- {event_type}: {actors}")

    context = "\n".join(context_lines)

    prompt = f"""
You are an analyst extracting conflict events from reports.

CONTEXT (recent similar events):
{context}

TEXT TO ANALYZE:
{raw}

Extract events as JSON. Use this exact schema:

{{
  "events": [
    {{
      "event_type": "string",
      "date": "YYYY-MM-DD or null",
      "location": "string or null",
      "actors": ["Actor A", "Actor B"],
      "fatalities": 0
    }}
  ],
  "confidence": 0.0
}}
"""

    response = llm.invoke([
        SystemMessage(content="Extract conflict events from the text. Output JSON ONLY."),
        HumanMessage(content=prompt),
    ])

    try:
        result = json.loads(response.content)
        state["extracted_events"] = result
        n = len(result.get("events", []))
        state["messages"].append(f"Extracted {n} events from raw_data")
    except Exception as e:
        state["extracted_events"] = {"error": "parse_failed", "raw": response.content}
        state["messages"].append(f"âš ï¸ Event extraction JSON parse failed: {e}")

    return state


def trend_analyst_node(state: SudanCRAMState) -> SudanCRAMState:
    """Analyze trends using the retrieved events timeline."""
    print("\nðŸ“Š Trend Analyst...")

    # Build a simple timeline summary
    timeline_lines = []
    for e in sorted(state.get("retrieved_events", []),
                    key=lambda x: x.get("metadata", {}).get("date", ""))[:15]:
        meta = e.get("metadata", {})
        date = meta.get("date", "unknown_date")
        event_type = meta.get("event_type", "unknown_type")
        fatalities = meta.get("fatalities", 0)
        timeline_lines.append(f"{date}: {event_type} ({fatalities} fatalities)")

    timeline = "\n".join(timeline_lines)

    prompt = f"""
You are a conflict trend analyst for Sudan.

REGION: {state['region']}

EVENT TIMELINE:
{timeline}

Analyze trends and produce JSON ONLY in this schema:

{{
  "trend_classification": "ESCALATING" | "STABLE" | "DEESCALATING" | "VOLATILE",
  "drivers": ["short bullet point string", "..."],
  "forecast_7_days": {{
    "armed_clash_likelihood": 0-100,
    "civilian_targeting_likelihood": 0-100
  }},
  "confidence": "LOW" | "MEDIUM" | "HIGH"
}}
"""

    response = llm.invoke([
        SystemMessage(content="Analyze conflict trends and output JSON ONLY."),
        HumanMessage(content=prompt),
    ])

    try:
        result = json.loads(response.content)
        state["trend_analysis"] = result
        state["messages"].append(
            f"Trend classification: {result.get('trend_classification', 'UNKNOWN')}"
        )
    except Exception as e:
        state["trend_analysis"] = {"error": "parse_failed", "raw": response.content}
        state["messages"].append(f"âš ï¸ Trend analysis JSON parse failed: {e}")

    return state


def scenario_generator_node(state: SudanCRAMState) -> SudanCRAMState:
    """Generate intervention scenarios."""
    print("\nðŸ”® Scenario Generator...")

    interventions = state.get("interventions")
    if not interventions:
        state["scenarios"] = None
        state["messages"].append("No interventions provided; skipping scenario generation")
        return state

    trend = state.get("trend_analysis")

    prompt = f"""
You are advising on conflict response options in {state['region']}.

CURRENT TREND (may be null):
{json.dumps(trend, ensure_ascii=False, indent=2)}

INTERVENTIONS TO EVALUATE:
{chr(10).join(f"- {i}" for i in interventions)}

Return JSON ONLY with this schema:

{{
  "scenarios": [
    {{
      "intervention": "string",
      "optimistic": {{
        "description": "string",
        "success_probability": 0-100
      }},
      "pessimistic": {{
        "description": "string",
        "risk_probability": 0-100
      }},
      "recommendation": "PROCEED" | "MODIFY" | "AVOID"
    }}
  ]
}}
"""

    response = llm.invoke([
        SystemMessage(content="Generate policy scenarios and output JSON ONLY."),
        HumanMessage(content=prompt),
    ])

    try:
        result = json.loads(response.content)
        state["scenarios"] = result
        n = len(result.get("scenarios", []))
        state["messages"].append(f"Generated {n} scenarios")
    except Exception as e:
        state["scenarios"] = {"error": "parse_failed", "raw": response.content}
        state["messages"].append(f"âš ï¸ Scenario generation JSON parse failed: {e}")

    return state


def consistency_checker_node(state: SudanCRAMState) -> SudanCRAMState:
    """Check for contradictions and produce an overall confidence score."""
    print("\nâœ… Consistency Checker...")

    payload = {
        "events": state.get("extracted_events"),
        "trends": state.get("trend_analysis"),
        "scenarios": state.get("scenarios"),
    }

    prompt = f"""
You are checking internal consistency of an analysis.

DATA:
{json.dumps(payload, ensure_ascii=False, indent=2)}

Identify contradictions or weak assumptions.

Return JSON ONLY:

{{
  "validation_status": "PASSED" | "WARNING" | "FAILED",
  "issues": [
    {{
      "type": "INCONSISTENCY" | "DATA_GAP",
      "description": "string",
      "severity": "LOW" | "MEDIUM" | "HIGH"
    }}
  ],
  "overall_confidence": 0.0-1.0
}}
"""

    response = llm.invoke([
        SystemMessage(content="Validate consistency and output JSON ONLY."),
        HumanMessage(content=prompt),
    ])

    try:
        result = json.loads(response.content)
        state["validation"] = result
        state["confidence_score"] = float(result.get("overall_confidence", 0.5))
        state["messages"].append(
           f"Validation: {result.get('validation_status', 'UNKNOWN')} "
           f"(confidence={state['confidence_score']:.2f})"
        )
    except Exception as e:
        state["validation"] = {"error": "parse_failed", "raw": response.content}
        state["confidence_score"] = 0.5
        state["messages"].append(f"âš ï¸ Validation JSON parse failed: {e}")

    return state


def human_approval_node(state: SudanCRAMState) -> SudanCRAMState:
    """Decide if human approval is required based on confidence."""
    print("\nðŸ‘¤ Human Approval Check...")

    conf = state.get("confidence_score", 0.0)

    if conf < 0.7:
        state["human_approval_required"] = True
        state["approval_status"] = "pending"
        state["messages"].append(
            f"âš ï¸ Human approval required (confidence={conf:.2f})"
        )
    else:
        state["human_approval_required"] = False
        state["approval_status"] = "auto-approved"
        state["messages"].append(
            f"âœ… Auto-approved (confidence={conf:.2f})"
        )

    return state


def should_request_human_input(state: SudanCRAMState) -> str:
    """Routing function for LangGraph conditional edge."""
    return "request_approval" if state.get("human_approval_required") else "finalize"
