# scripts/gdelt/analyze_goldstein_trends.py
"""
Analyze Goldstein Scale trends to detect escalation
Uses the CSV file generated by fetch_sudan_events.py
"""
import pandas as pd
import numpy as np
from datetime import datetime
import os
import glob

def load_latest_gdelt_data():
    """Load the most recent GDELT Sudan events CSV"""
    gdelt_files = glob.glob('data/gdelt/sudan_events_*.csv')
    
    if not gdelt_files:
        raise FileNotFoundError("No GDELT data found. Run fetch_sudan_events.py first!")
    
    # Get most recent file
    latest_file = max(gdelt_files, key=os.path.getctime)
    print(f"üìÇ Loading: {latest_file}")
    
    df = pd.read_csv(latest_file)
    df['date'] = pd.to_datetime(df['date'])
    
    return df

def calculate_escalation_risk(df):
    """
    Calculate escalation risk scores by location
    Based on Goldstein trends, event frequency, and media attention
    """
    print("\nüîç Analyzing escalation patterns...\n")
    
    # Clean location names (some have duplicates due to spelling)
    df['location_clean'] = df['location'].str.strip()
    
    # Group by location
    location_stats = []
    
    for location in df['location_clean'].unique():
        loc_data = df[df['location_clean'] == location].sort_values('date')
        
        if len(loc_data) < 2:
            continue
        
        # Calculate metrics
        avg_goldstein = loc_data['goldstein'].mean()
        min_goldstein = loc_data['goldstein'].min()
        event_count = len(loc_data)
        media_mentions = loc_data['num_mentions'].sum()
        
        # Goldstein trend (negative = escalating)
        goldstein_trend = loc_data['goldstein'].diff().mean()
        
        # Recent vs older (split data in half)
        split_point = len(loc_data) // 2
        recent_avg = loc_data.iloc[split_point:]['goldstein'].mean()
        older_avg = loc_data.iloc[:split_point]['goldstein'].mean()
        change = recent_avg - older_avg
        
        # Escalation Risk Score (0-10)
        # Higher = more dangerous
        risk_score = (
            max(0, -avg_goldstein) * 0.4 +        # Negative Goldstein = conflict
            max(0, -goldstein_trend) * 0.3 +      # Declining trend = escalation
            min(10, event_count / 5) * 0.2 +      # More events = more attention
            max(0, -change) * 0.1                 # Getting worse over time
        )
        
        # Classify risk level
        if risk_score >= 7:
            risk_level = "CRITICAL"
        elif risk_score >= 5:
            risk_level = "HIGH"
        elif risk_score >= 3:
            risk_level = "MODERATE"
        else:
            risk_level = "LOW"
        
        location_stats.append({
            'location': location,
            'escalation_risk': min(10, risk_score),
            'risk_level': risk_level,
            'avg_goldstein': avg_goldstein,
            'goldstein_trend': goldstein_trend,
            'event_count': event_count,
            'media_mentions': media_mentions,
            'recent_change': change,
            'first_seen': loc_data['date'].min(),
            'last_seen': loc_data['date'].max()
        })
    
    risk_df = pd.DataFrame(location_stats).sort_values('escalation_risk', ascending=False)
    
    return risk_df

def generate_hourly_timeline(df):
    """Generate hourly Goldstein timeline for charts"""
    df['hour'] = df['date'].dt.floor('H')
    
    hourly = df.groupby('hour').agg({
        'goldstein': 'mean',
        'event_code': 'count',
        'num_mentions': 'sum'
    }).reset_index()
    
    hourly.columns = ['timestamp', 'avg_goldstein', 'event_count', 'mentions']
    
    return hourly

def main():
    print("=" * 70)
    print("GOLDSTEIN SCALE TREND ANALYSIS")
    print("=" * 70)
    
    # Load data
    df = load_latest_gdelt_data()
    print(f"‚úÖ Loaded {len(df)} events")
    print(f"   Date range: {df['date'].min()} ‚Üí {df['date'].max()}\n")
    
    # Calculate escalation risks
    risk_df = calculate_escalation_risk(df)
    
    # Save results
    os.makedirs('data/processed', exist_ok=True)
    risk_file = f'data/processed/goldstein_escalation_risk_{datetime.now().strftime("%Y%m%d")}.csv'
    risk_df.to_csv(risk_file, index=False)
    
    # Display results
    print("=" * 70)
    print("üö® ESCALATION RISK RANKING (Top 10)")
    print("=" * 70)
    
    for i, row in risk_df.head(10).iterrows():
        print(f"\n{i+1}. {row['location']}")
        print(f"   Risk Score:      {row['escalation_risk']:.1f}/10 [{row['risk_level']}]")
        print(f"   Avg Goldstein:   {row['avg_goldstein']:.2f} (negative = conflict)")
        print(f"   Trend:           {row['goldstein_trend']:.2f} (negative = worsening)")
        print(f"   Events:          {row['event_count']} events")
        print(f"   Media mentions:  {row['media_mentions']}")
        print(f"   Recent change:   {row['recent_change']:.2f}")
    
    # Generate hourly timeline
    timeline = generate_hourly_timeline(df)
    timeline_file = f'data/processed/goldstein_hourly_timeline_{datetime.now().strftime("%Y%m%d")}.csv'
    timeline.to_csv(timeline_file, index=False)
    
    print("\n" + "=" * 70)
    print("üìä KEY INSIGHTS")
    print("=" * 70)
    
    critical = risk_df[risk_df['risk_level'] == 'CRITICAL']
    high = risk_df[risk_df['risk_level'] == 'HIGH']
    
    print(f"Critical risk locations:  {len(critical)}")
    print(f"High risk locations:      {len(high)}")
    print(f"Total locations tracked:  {len(risk_df)}")
    
    print(f"\nüíæ Files saved:")
    print(f"   ‚Ä¢ {risk_file}")
    print(f"   ‚Ä¢ {timeline_file}")
    
    print("\n" + "=" * 70)
    print("‚úÖ ANALYSIS COMPLETE")
    print("=" * 70)
    
    return risk_df, timeline

if __name__ == '__main__':
    risk_df, timeline = main()
